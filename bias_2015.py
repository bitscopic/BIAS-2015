"""
Take in an Illumina Connected Annotations (aka NIRVANA) .json file and apply ACMG standards using
the BIAS-2015 algorithm to classify the variants.

BIAS-2015 requires multiple data files to run. The user must pass these to the algorithm through
a required_paths.json file that tells the algorithm the full path for each of these required files.

The data files can be downloaded from the Bitscopic AWS bucket



and unzipped. Then a required_paths.json file can be made by running the create_new_required_paths_file.py script 

python3 src/scripts/create_new_required_paths_file.py <file_directory> hg19 hg19_required_paths.json

Alternatively, the required data files can be generated by running the preprocessing.py script. Please note 
this takes multiple hours, and will write many GB to disk. It will create the required files, along with
a required_paths.json file. 
"""
import json
import sys
import os
import io
import gzip
import argparse
import time
import logging
from src.bias_2015 import extract_from_nirvana_json, bias_variant_classification, bias_dataset_loader, constants
    
# Keep track of runtime
start_time = time.time()

def parseArgs(): 
    """
    Parse the command line arguments into useful python objects.  '--' variables are optional
    set_defaults only applies when the argument is not provided (it won't override)
    """
    parser = argparse.ArgumentParser(description = __doc__)
    parser.add_argument("nirvana_json_file",
                        help = " Nirvana (or ICA) .json or .json.gz output file",
                        action = "store")
    parser.add_argument("file_paths_json",
                        help = " .json file describing where BIAS additional input files are located",
                        action = "store")
    parser.add_argument("output_file",
                        help = " ACMG classification tsv file",
                        action = "store")
    parser.add_argument("--user_classifiers",
                        help = " User provided classifiers for each variant",
                        action = "store")
    parser.add_argument("--skip_list",
                        help = " A file listing the ACMG codes to be skipped. One code per line. Ex. providing 'PP5' will turn off the ClinVar check for pathogenicity",
                        action = "store")
    parser.add_argument("--verbose",
                        help = " The verbosity level for stdout messages (default INFO)",
                        choices=["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"],
                        action = "store")

    parser.set_defaults(verbose = "WARNING")
    options = parser.parse_args()
    logging.basicConfig(level=getattr(logging, options.verbose), format='%(message)s')
    return options


def open_file(file_path, mode):
    """
    Open either a normal or a .gz file
    """
    try:
        _, file_extension = os.path.splitext(file_path)
        if file_extension == ".gz":
            return gzip.open(file_path, mode)
        return io.open(file_path, mode, encoding="utf-8")
    except FileNotFoundError:
        logging.critical("File not found: %s. Please check the file path.", file_path)
        sys.exit(1)
    except IOError as e:
        logging.critical("IO error while accessing file: %s. Error: %s", file_path, e)
        sys.exit(1)


def classify_variants(output_file, nirvana_json_file, name_to_dataset, variant_to_user_classification, skip_list):
    """
    Stream through the input file, classify variants, and write results with error handling.
    """
    v_count = 0
    try:
        with open(output_file, 'w') as o_file:
            headers = ["chromosome", "position", "refAllele", "altAllele", "variantType", "consequence", 
                       "acmgClassification", "alleleFreq", "hgvsg", "hgvsc", "hgvsp", "aaChange", "geneName", 
                       "pubmedIds", "associatedDiseases", "dbSnpids", "transcript", "rationale"]
            o_file.write("\t".join(headers) + "\n")

            with open_file(nirvana_json_file, "rt") as f:
                _ = f.readline()[10:-15]
                genes_section = False
                for line in f:
                    if genes_section:
                        break
                    if '"genes":[' in line:
                        genes_section = True
                        continue
                    if line.startswith("]}"):
                        continue
                    try:
                        data = json.loads(line[:-2] if line[-2] == "," else line)
                        chrom = data["chromosome"]
                        pos = str(data["position"])
                        ref = data["refAllele"]
                        if not data.get("altAlleles"):
                            continue # Minor reference alleles - not supported currently
                        alt = data["altAlleles"][0]
                        for variant in data["variants"]:
                            single_variant = extract_from_nirvana_json.process_variant(
                                variant, name_to_dataset['hgnc_to_gene_data'], "RefSeq", chrom, pos, ref, alt)
                            variant_key = (chrom, pos, single_variant.refAllele, single_variant.altAllele)
                            #Get supplement user classification, if any
                            supplemental_codes = variant_to_user_classification.get(variant_key, {})
                            
                            #append the classification to the variant object (significance, justification)
                            single_variant.significance, single_variant.justification = \
                                bias_variant_classification.get_variant_classification(single_variant, name_to_dataset, supplemental_codes, skip_list)
                            v_count += 1
                            o_file.write(single_variant.to_tsv() + "\n")
                    except KeyError as e:
                        logging.error("Missing key in JSON data: %s. Error: %s", line.strip(), e)
                        raise
                    except Exception as e:
                        logging.error("Unexpected error while processing variant: %s. Error: %s", line.strip(), e)
                        raise
                    if v_count % 25000 == 0:
                        logging.debug("Processed %d variants...", v_count)

        logging.info("Processed %d total variants in %d genes", v_count, len(name_to_dataset['hgnc_to_gene_data']))
    except Exception as e:
        raise
        logging.critical("Error during variant classification. Output file: %s. Error: %s", output_file, e)
        sys.exit(1)


def run_bias(options):
    """
    Load user provided classifications if provided. Load in the data files needed for classification

    Stream through the ICA/NIRVANA json file, keeping only one json element in memory at a time. 

    Classify each variant one at a time, and write it to the output file as it is being classified. 
    """
    # Load the core paths
    config = {}
    with open(options.file_paths_json,'r') as config_file:
        config = json.load(config_file)

    # User provided classifications (optional)
    variant_to_user_classification = {}
    if options.user_classifiers:
        logging.warning("User classifications detected!")
        with open(options.user_classifiers, 'r') as in_file:
            in_file.readline()
            for line in in_file:
                split_line = line.strip().split("\t")
                chrom, pos, ref, alt = split_line[:4]
                justification = json.loads(split_line[-1]) # The justification is the final column of the output
                variant = (chrom, pos, ref, alt)
                variant_to_user_classification[variant] = justification 
    
    # Check for codes that should be skipped...
    skip_list = set()
    bad_skips = set()
    if options.skip_list:
        with open(options.skip_list, 'r') as in_file:
            for line in in_file:
                user_code = line.strip().lower() # remove new lines and make it lower case
                if user_code in constants.evaluated_codes:
                    skip_list.add(user_code)
                else:
                    bad_skips.add(user_code)
    if bad_skips:
        bad_skip_str = ", ".join(bad_skips)
        logging.warning("User provdied code(s) to skip '%s', however this code(s) is not currently evaluated by BIAS", bad_skip_str)
    if skip_list:
        skip_list = list(skip_list)
        skipped_code_string = ", " .join(skip_list)
        logging.info("Excluding %s code(s) by user request", skipped_code_string)

    # Load in the datasets needed for classification
    ###### NOTE: This step loads a lot of data into memory, it can take up to ~5 GB of RAM ########
    print("Loading data sets...")
    name_to_dataset = bias_dataset_loader.get_name_to_dataset(config)
    
    # Gather the NIRVANA gene information
    hgnc_to_gene_data = extract_from_nirvana_json.load_nirvana_gene_information(options.nirvana_json_file) 
    name_to_dataset['hgnc_to_gene_data'] = hgnc_to_gene_data

    end_time = time.time()
    load_time = end_time - start_time
    logging.info("Time to load datasets: %.4f seconds", load_time)
  
    # Stream through the NIRVANA annotation json file and classify each variant, writing it to the output file
    print("Classifying variants...")
    classify_variants(options.output_file, options.nirvana_json_file, name_to_dataset, variant_to_user_classification, skip_list)
    
    end_time = time.time()
    run_time = end_time - start_time - load_time
    logging.info("Time to classify variants: %.4f seconds", run_time)


def main():
    """
    Go through Nirvana json and perform calculations
    """
    # Parse arguments 
    options = parseArgs()
    
    # Run the BIAS algorithm
    run_bias(options)
    
    # Calculate wall runtime
    end_time = time.time()
    total_runtime = end_time - start_time
    
    # Convert to hours, minutes, seconds
    hours, remainder = divmod(total_runtime, 3600)
    minutes, seconds = divmod(remainder, 60)

    # Build the runtime string dynamically
    runtime_parts = []
    if hours > 0:
        runtime_parts.append(f"{int(hours)}h")
    if minutes > 0:
        runtime_parts.append(f"{int(minutes)}m")
    runtime_parts.append(f"{seconds:.2f}s")  # Seconds are always included
    runtime_str = " ".join(runtime_parts)
    print(f"Finished! Total BIAS-2015 runtime was {runtime_str}")

if __name__ == "__main__":
    sys.exit(main())
